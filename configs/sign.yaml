name: sign_experiment
data:
  img_path: ../../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/
  version: phoenix_2014_trans
  sgn: sign
  txt: text
  gls: gloss
  train_path: ./data/Phonexi-2014T/labels.train
  dev_path: ./data/Phonexi-2014T/labels.dev
  test_path: ./data/Phonexi-2014T/labels.test
  feature_size: 512
  level: word
  txt_lowercase: true
  max_sent_length: 400
  random_train_subset: -1
  random_dev_subset: -1
  multimodal: 0.0
  max_length: 500
testing:
  recognition_beam_sizes:
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
    - 9
    - 10
  translation_beam_sizes:
    - 1
    - 2
    - 3
    - 4
    - 5
    - 6
    - 7
    - 8
    - 9
    - 10
  translation_beam_alphas:
    - -1
    - 0
    - 1
    - 2
    - 3
    - 4
    - 5
training:
  reset_best_ckpt: false
  reset_scheduler: false
  reset_optimizer: false
  random_seed: 42
  model_dir: "./sign_sample"
  recognition_loss_weight: 0.0
  translation_loss_weight: 1.0
  eval_metric: bleu
  optimizer: sophiag
  learning_rate: 0.0004
  batch_size: 2
  num_valid_log: 6
  epochs: 1000
  early_stopping_metric: eval_metric
  batch_type: sentence
  translation_normalization: batch
  eval_translation_beam_size: 1
  eval_translation_beam_alpha: -1
  overwrite: true
  shuffle: true
  use_cuda: true
  translation_max_output_length: 30
  keep_last_ckpts: 1
  batch_multiplier: 1
  logging_freq: 100
  validation_freq: 10
  betas:
    - 0.95
    - 0.998
  scheduling: plateau
  learning_rate_min: 1.0e-06
  weight_decay: 0.003
  patience: 10
  decrease_factor: 0.8
  label_smoothing: 0.1
  lr_s_dim_model: 256
  warmup_step: 1000
  K: 2
model:
  initializer: xavier
  bias_initializer: zeros
  init_gain: 1.0
  embed_initializer: xavier
  embed_init_gain: 1.0
  tied_softmax: false
  cope: false
  encoder:
    type: transformer
    num_layers: 1
    num_heads: 8
    embeddings:
      embedding_dim: 256
      scale: false
      dropout: 0.1
      norm_type: batch
      activation_type: softsign
    hidden_size: 256
    ff_size: 1024
    dropout: 0.1
  decoder:
    type: transformer
    num_layers: 1
    num_heads: 8
    embeddings:
      embedding_dim: 256
      scale: false
      dropout: 0.1
      norm_type: batch
      activation_type: softsign
    hidden_size: 256
    ff_size: 1024
    dropout: 0.1
