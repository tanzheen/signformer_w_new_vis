from typing import Optional

import torch
import torch.nn as nn
from torch import Tensor
from encoders import Encoder 

from attention import BahdanauAttention, LuongAttention
from helpers import freeze_params, subsequent_mask
from transformer_layers import  TransformerDecoderLayer
from position_encoding import PositionalEncoding, CoPE 

class Decoder(nn.Module):
    """
    Base decoder class
    """

    @property
    def output_size(self):
        """
        Return the output size (size of the target vocabulary)

        :return:
        """
        return self._output_size
    

class TransformerDecoder(Decoder): 
    """
    A transformer decoder with N masked layers.
    Decoder layers are masked so that an attention head cannot see the future.
    """

    def __init__(
        self,
        num_layers: int = 4,
        num_heads: int = 8,
        hidden_size: int = 512,
        ff_size: int = 2048,
        dropout: float = 0.1,
        emb_dropout: float = 0.1,
        vocab_size: int = 1,
        freeze: bool = False,
        cope=False,
        **kwargs
    ):
        """
        Initialize a Transformer decoder.

        :param num_layers: number of Transformer layers
        :param num_heads: number of heads for each layer
        :param hidden_size: hidden size
        :param ff_size: position-wise feed-forward size
        :param dropout: dropout probability (1-keep)
        :param emb_dropout: dropout probability for embeddings
        :param vocab_size: size of the output vocabulary
        :param freeze: set to True keep all decoder parameters fixed
        :param kwargs:
        """
        super(TransformerDecoder, self).__init__()
        self._hidden_size = hidden_size
        self._output_size = vocab_size

        # create num_layers decoder layers and put them in a list
        self.layers = nn.ModuleList(
            [
                TransformerDecoderLayer(
                    size=hidden_size,
                    ff_size=ff_size,
                    num_heads=num_heads,
                    dropout=dropout,
                    cope=cope,
                )
                for _ in range(num_layers)
            ]
        )
        self.pe = PositionalEncoding(hidden_size)
        self.layer_norm = nn.LayerNorm(hidden_size, eps=1e-6)

        self.tokenTypeEmbedding = nn.Embedding(2, hidden_size)

        self.emb_dropout = nn.Dropout(p=emb_dropout)
        self.output_layer = nn.Linear(hidden_size, vocab_size, bias=False)

        if freeze:
            freeze_params(self)

    def forward(
        self,
        trg_embed: Tensor = None,
        encoder_output: Tensor = None,
        encoder_hidden: Tensor = None,
        src_mask: Tensor = None,
        unroll_steps: int = None,
        hidden: Tensor = None,
        trg_mask: Tensor = None,
        **kwargs
    ):
            """
            Transformer decoder forward pass.

            :param trg_embed: embedded targets
            :param encoder_output: source representations
            :param encoder_hidden: unused
            :param src_mask:
            :param unroll_steps: unused
            :param hidden: unused
            :param trg_mask: to mask out target paddings
                            Note that a subsequent mask is applied here.
            :param kwargs:
            :return:
            """
            assert trg_mask is not None, "trg_mask required for Transformer"
            x = self.pe(trg_embed)
            x = self.emb_dropout(x)
            trg_mask = trg_mask & subsequent_mask(trg_embed.size(1)).type_as(trg_mask)
            encoder_output = self.emb_dropout(encoder_output)
            for layer in self.layers: 
                x = layer(x=x, memory=encoder_output, src_mask=src_mask, trg_mask=trg_mask)
            
            x = self.layer_norm(x)
            output = self.output_layers(x)

            return output, x

        